name: Update patches.json

on:
  workflow_dispatch:
  schedule:
    - cron: "12,42 * * * *" # every 30 minutes (UTC)

permissions:
  contents: write

concurrency:
  group: "update-dataset"
  cancel-in-progress: true

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download latest patches.json
        run: |
          set -euo pipefail

          # Ensure we're on an up-to-date local main branch before writing files.
          # Scheduled runs can overlap with other pushes (including other dataset runs).
          git checkout -B main
          git pull --rebase origin main

          SRC_URL="https://downloads.esri.com/patch_notification/patches.json"
          curl -fsSL "$SRC_URL" -o "patches.json"

          # Write metadata for display in the UI.
          # Note: updated_at_utc is the time this workflow ran (UTC).
          UPDATED_AT_UTC="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
          export SRC_URL UPDATED_AT_UTC
          python3 - <<'PY'
          import hashlib
          import json
          import os
          from pathlib import Path

          p = Path("patches.json").read_bytes()
          meta = {
            "source_url": os.environ.get("SRC_URL", ""),
            "updated_at_utc": os.environ["UPDATED_AT_UTC"],
            "patches_sha256": hashlib.sha256(p).hexdigest(),
            "bytes": len(p),
          }
          Path("patches.meta.json").write_text(json.dumps(meta, indent=2, sort_keys=True) + "\n")
          PY
          echo "updated_at_utc=$UPDATED_AT_UTC"

      - name: Generate sitemaps
        run: |
          set -euo pipefail
          python3 - <<'PY'
          import json
          import re
          from dataclasses import dataclass
          from datetime import date
          from html import escape
          from pathlib import Path
          from urllib.parse import quote

          BASE_URL = "https://ceddc.github.io/simple-patch-finder/"
          PATCHES_JSON = Path("patches.json")
          PATCHES_META_JSON = Path("patches.meta.json")
          SITEMAP_INDEX_XML = Path("sitemap.xml")
          SITEMAP_PAGES_XML = Path("sitemap-pages.xml")
          SITEMAP_PATCHES_XML = Path("sitemap-patches.xml")


          @dataclass(frozen=True)
          class UrlEntry:
              loc: str
              lastmod: str = ""


          def write_text_lf(path: Path, content: str) -> None:
              with path.open("w", encoding="utf-8", newline="\n") as f:
                  f.write(content)


          def slugify_patch_name(name: str) -> str:
              s = (name or "").lower()
              s = s.replace("&", " and ")
              s = re.sub(r"[^a-z0-9]+", "-", s)
              s = re.sub(r"(^-+|-+$)", "", s)
              return s[:180]


          def tokenize_csv(value: str) -> list[str]:
              return [t.strip() for t in (value or "").split(",") if t.strip()]


          def parse_release_date(value: str) -> str:
              m = re.match(r"^(\d{1,2})/(\d{1,2})/(\d{4})$", (value or "").strip())
              if not m:
                  return ""
              mm, dd, yyyy = int(m.group(1)), int(m.group(2)), int(m.group(3))
              try:
                  return date(yyyy, mm, dd).isoformat()
              except ValueError:
                  return ""


          def read_dataset_lastmod() -> str:
              if not PATCHES_META_JSON.exists():
                  return ""
              try:
                  meta = json.loads(PATCHES_META_JSON.read_text(encoding="utf-8"))
                  return str(meta.get("updated_at_utc", "")).strip()
              except Exception:
                  return ""


          def build_entries() -> tuple[list[UrlEntry], list[UrlEntry], str]:
              raw = json.loads(PATCHES_JSON.read_text(encoding="utf-8"))
              groups = raw.get("Product") if isinstance(raw, dict) else []
              groups = groups if isinstance(groups, list) else []

              dataset_lastmod = read_dataset_lastmod()
              product_lastmods: dict[str, str] = {}
              patch_entries: dict[str, UrlEntry] = {}

              for g in groups:
                  patches = g.get("patches") if isinstance(g, dict) else []
                  patches = patches if isinstance(patches, list) else []
                  for p in patches:
                      if not isinstance(p, dict):
                          continue

                      rel = parse_release_date(str(p.get("ReleaseDate", "")))

                      for prod in tokenize_csv(str(p.get("Products", ""))):
                          if not prod:
                              continue
                          prev = product_lastmods.get(prod, "")
                          if rel and (not prev or rel > prev):
                              product_lastmods[prod] = rel
                          elif not prev and dataset_lastmod:
                              product_lastmods[prod] = dataset_lastmod

                      pid = str(p.get("QFE_ID", "")).strip()
                      name = str(p.get("Name", "")).strip()
                      if not pid:
                          continue

                      pn = slugify_patch_name(name)
                      loc = f"{BASE_URL}?pid={quote(pid, safe='')}&pn={quote(pn, safe='')}"
                      lastmod = rel or dataset_lastmod
                      existing = patch_entries.get(loc)
                      if existing is None:
                          patch_entries[loc] = UrlEntry(loc=loc, lastmod=lastmod)
                      elif lastmod and (not existing.lastmod or lastmod > existing.lastmod):
                          patch_entries[loc] = UrlEntry(loc=loc, lastmod=lastmod)

              page_entries = [UrlEntry(loc=BASE_URL, lastmod=dataset_lastmod)]
              for prod in sorted(product_lastmods):
                  loc = f"{BASE_URL}?p={quote(prod, safe='')}"
                  page_entries.append(
                      UrlEntry(loc=loc, lastmod=product_lastmods.get(prod, "") or dataset_lastmod)
                  )

              patch_list = [patch_entries[k] for k in sorted(patch_entries)]
              return page_entries, patch_list, dataset_lastmod


          def write_urlset(path: Path, entries: list[UrlEntry]) -> None:
              lines = [
                  '<?xml version="1.0" encoding="UTF-8"?>',
                  '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">',
              ]
              for e in entries:
                  lines.append("  <url>")
                  lines.append(f"    <loc>{escape(e.loc, quote=False)}</loc>")
                  if e.lastmod:
                      lines.append(f"    <lastmod>{escape(e.lastmod, quote=False)}</lastmod>")
                  lines.append("  </url>")
              lines.append("</urlset>")
              write_text_lf(path, "\n".join(lines) + "\n")


          def write_sitemap_index(dataset_lastmod: str) -> None:
              items = [
                  UrlEntry(loc=f"{BASE_URL}sitemap-pages.xml", lastmod=dataset_lastmod),
                  UrlEntry(loc=f"{BASE_URL}sitemap-patches.xml", lastmod=dataset_lastmod),
              ]

              lines = [
                  '<?xml version="1.0" encoding="UTF-8"?>',
                  '<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">',
              ]
              for item in items:
                  lines.append("  <sitemap>")
                  lines.append(f"    <loc>{escape(item.loc, quote=False)}</loc>")
                  if item.lastmod:
                      lines.append(f"    <lastmod>{escape(item.lastmod, quote=False)}</lastmod>")
                  lines.append("  </sitemap>")
              lines.append("</sitemapindex>")
              write_text_lf(SITEMAP_INDEX_XML, "\n".join(lines) + "\n")


          pages, patches, dataset_lastmod = build_entries()
          write_urlset(SITEMAP_PAGES_XML, pages)
          write_urlset(SITEMAP_PATCHES_XML, patches)
          write_sitemap_index(dataset_lastmod)
          PY

      - name: Commit if changed
        run: |
          set -euo pipefail
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # Stage the file even if it was previously untracked.
          git add patches.json patches.meta.json sitemap.xml sitemap-pages.xml sitemap-patches.xml

          if git diff --cached --quiet; then
            echo "No dataset/sitemap changes"
            exit 0
          fi

          git commit -m "chore: update dataset"

          # Try to push; if the remote advanced since our pull, rebase and retry once.
          git push origin main || (git pull --rebase origin main && git push origin main)
